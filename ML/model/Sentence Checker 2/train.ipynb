{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 3.066265821456909\n",
      "Validation loss: 3.0471693674723306\n",
      "Epoch 2/5\n",
      "Training loss: 2.246285915374756\n",
      "Validation loss: 2.1865765121248035\n",
      "Epoch 3/5\n",
      "Training loss: 1.4804396629333496\n",
      "Validation loss: 1.4781555599636502\n",
      "Epoch 4/5\n",
      "Training loss: 0.9485369920730591\n",
      "Validation loss: 1.1375425789091322\n",
      "Epoch 5/5\n",
      "Training loss: 0.8191072940826416\n",
      "Validation loss: 0.8825726442866855\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_json('Intent.json')\n",
    "sentences = [item for sublist in data['intents'] for item in sublist['text']]\n",
    "labels = [idx for idx, sublist in enumerate(data['intents']) for _ in sublist['text']]\n",
    "\n",
    "# Encode labels\n",
    "num_labels = len(data['intents'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=num_labels)\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# Load model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    train_labels\n",
    ")).shuffle(len(train_texts)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']},\n",
    "    val_labels\n",
    ")).batch(16)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # Training\n",
    "    for batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(batch[0], training=True)\n",
    "            loss = loss_fn(batch[1], outputs.logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(f\"Training loss: {loss.numpy()}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    for batch in val_dataset:\n",
    "        outputs = model(batch[0], training=False)\n",
    "        val_loss += loss_fn(batch[1], outputs.logits).numpy()\n",
    "    val_loss /= len(val_dataset)\n",
    "    print(f\"Validation loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.8825726442866855, Accuracy: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "val_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for batch in val_dataset:\n",
    "    outputs = model(batch[0], training=False)\n",
    "    val_loss += loss_fn(batch[1], outputs.logits).numpy()\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(outputs.logits, axis=1)\n",
    "    labels = tf.argmax(batch[1], axis=1)\n",
    "    correct_predictions += tf.reduce_sum(tf.cast(predictions == labels, tf.float32)).numpy()\n",
    "    total_predictions += labels.shape[0]\n",
    "\n",
    "val_loss /= len(val_dataset)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Validation loss: {val_loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: Loss of Taste\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model with a single input\n",
    "def test_model(sentence):\n",
    "    # Tokenize the input\n",
    "    encoding = tokenizer(sentence, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "    outputs = model(encoding)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    return predicted_class\n",
    "\n",
    "# Test an example sentence\n",
    "example_sentence = \"The lack of taste is making it hard to enjoy anything, even drinks.\"\n",
    "predicted_class = test_model(example_sentence)\n",
    "\n",
    "# Map the predicted class to the intent\n",
    "intent = data['intents'][predicted_class]['intent']\n",
    "print(f\"Predicted intent: {intent}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
