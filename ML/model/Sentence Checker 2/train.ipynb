{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training loss: 3.3273134231567383\n",
      "Validation loss: 3.3252113130357532\n",
      "Epoch 2/5\n",
      "Training loss: 2.5487217903137207\n",
      "Validation loss: 2.4756075541178384\n",
      "Epoch 3/5\n",
      "Training loss: 1.622786283493042\n",
      "Validation loss: 1.7161619398328993\n",
      "Epoch 4/5\n",
      "Training loss: 1.29448401927948\n",
      "Validation loss: 1.2113480303022597\n",
      "Epoch 5/5\n",
      "Training loss: 0.7105638384819031\n",
      "Validation loss: 0.943240225315094\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_json('Intent.json')\n",
    "sentences = [item for sublist in data['intents'] for item in sublist['text']]\n",
    "labels = [idx for idx, sublist in enumerate(data['intents']) for _ in sublist['text']]\n",
    "\n",
    "# Encode labels\n",
    "num_labels = len(data['intents'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=num_labels)\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# Load model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(learning_rate=5e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask']},\n",
    "    train_labels\n",
    ")).shuffle(len(train_texts)).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask']},\n",
    "    val_labels\n",
    ")).batch(16)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # Training\n",
    "    for batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(batch[0], training=True)\n",
    "            loss = loss_fn(batch[1], outputs.logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(f\"Training loss: {loss.numpy()}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    for batch in val_dataset:\n",
    "        outputs = model(batch[0], training=False)\n",
    "        val_loss += loss_fn(batch[1], outputs.logits).numpy()\n",
    "    val_loss /= len(val_dataset)\n",
    "    print(f\"Validation loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.943240225315094, Accuracy: 0.7651515151515151\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "val_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for batch in val_dataset:\n",
    "    outputs = model(batch[0], training=False)\n",
    "    val_loss += loss_fn(batch[1], outputs.logits).numpy()\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(outputs.logits, axis=1)\n",
    "    labels = tf.argmax(batch[1], axis=1)\n",
    "    correct_predictions += tf.reduce_sum(tf.cast(predictions == labels, tf.float32)).numpy()\n",
    "    total_predictions += labels.shape[0]\n",
    "\n",
    "val_loss /= len(val_dataset)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Validation loss: {val_loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: Loss of Taste\n"
     ]
    }
   ],
   "source": [
    "# Function to test the model with a single input\n",
    "def test_model(sentence):\n",
    "    # Tokenize the input\n",
    "    encoding = tokenizer(sentence, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "    outputs = model(encoding)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    return predicted_class\n",
    "\n",
    "# Test an example sentence\n",
    "example_sentence = \"The lack of taste is making it hard to enjoy anything, even drinks.\"\n",
    "predicted_class = test_model(example_sentence)\n",
    "\n",
    "# Map the predicted class to the intent\n",
    "intent = data['intents'][predicted_class]['intent']\n",
    "print(f\"Predicted intent: {intent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: Loss of Taste\n",
      "Model and label mapping saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Map the predicted class to the intent\n",
    "intent = data['intents'][predicted_class]['intent']\n",
    "print(f\"Predicted intent: {intent}\")\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')\n",
    "\n",
    "# Pickle the label mappings\n",
    "label_mapping = {i: intent['intent'] for i, intent in enumerate(data['intents'])}\n",
    "with open('label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"Model and label mapping saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
